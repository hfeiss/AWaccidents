{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Sklearn example from https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "from tokenator import tokenize_and_lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from the 20 news groups data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "< p > N o n - W i t n e s s   N a r r a t i v e   b y   c c w   o n   2 0 0 8 - 0 1 - 1 4   ( o k a y   t o   p u b l i s h ) :     A c o t t   L o v e l a n d   I n j u r e d   o n   S l i d e s   o f   M e a d o w   C r e e k ,   O h i o p y l e   P A     P o s t e d :   S a t   J a n   1 2 ,   2 0 0 8   8 : 3 9   p m       B e s t   W i s h e s   F o r   A   S p e e d y   R e c o v e r y   T o   S c o t   L o v e l a n d       I   w a n t e d   t o   u p d a t e   e v e r y o n e   o n   S c o t t ' s   s t a t u s ,   t o   s t o p   a n y   r u m o r s ,   b r i n g   e v e r y o n e   u p   t o   s p e e d ,   a n d   a l l o w   e v e r y o n e   t o   s e n d   t h e i r   b e s t   h o p e s   a n d   p r a y e r s   o n   f o r   h i s   s p e e d y   r e c o v e r y .     T o d a y ,   1 / 1 2 / 0 8   S c o t t   f l i p p e d   o v e r   i n   t h e   C l a s s   V   S l i d e s   r a p i d   o f   M e a d o w   R u n   i n   O h i o p y l e   a n d   s u s t a i n e d   a   s h a r p   b l o w   t o   h i s   h e a d   a n d / o r   n e c k .   H e   w a s   r e c o v e r e d   b y   b o a t e r s   i n   t h e   p o o l   a t   t h e   b o t t o m ,   w h e r e   h e   c o m p l a i n e d   o f   n e c k   p a i n   a n d   n u m b n e s s   i n   h i s   r i g h t   a r m .   P a r a m e d i c s   w e r e   c a l l e d   t o   t h e   s c e n e ,   a n d   i t   w a s   d e c i d e d   t h a t   h e   w o u l d   b e   l i f e   f l i g h t e d   t o   a   t r a u m a   h o s p i t a l   i n   P i t t s b u r g h .     T h e r e   i s   g o o d   a n d   b a d   n e w s   t o   f o l l o w :   T h e   b a d   i s   t h a t   S c o t t   s u s t a i n e d   a   f r a c t u r e   t o   o n e   o f   t h e   v e r t e b r a e   i n   h i s   n e c k .   T h e   g o o d   n e w s   i s   t h a t   h e   h a s   f e e l i n g   a n d   m o t i o n   i n   a l l   o f   l i m b s .   T h e y   a r e   w a i t i n g   f o r   a n   M R I   t o   d e c i d e   h o w   t o   p r o g r e s s   w i t h   h i s   t r e a t m e n t   f r o m   h e r e .     O u r   t h o u g h t s   g o   o u t   t o   h i m   a n d   h i s   f a m i l y   i n   t h e   t r o u b l e d   t i m e s   a h e a d ,   m a y   y o u   h a v e   a   s p e e d y   r e c   o v e r y .   O u r   t h o u g h t s   a l s o   g o   o u t   t o   J e f f ,   D e r e k   a n d   J e s s e   w h o s e   q u i c k   a n d   a p p r o p r i a t e   a c t i o n   o n   t h e   s c e n e   l i k e l y   a i d e d   i n   m a i n t a i n i n g   S c o t t ' s   m e n t a l   a n d   p h y s i c a l   w h e r e - w i t h - a l l   f o l l o w i n g   t h e   a c c i d e n t . < / p >   M e a d o w   R u n     S l i d e s   a b o v e   3 8 1   B r i d g e     O t h e r\n"
    }
   ],
   "source": [
    "df = pd.read_pickle('/Users/hfeiss/dsi/capstone-2/data/clean/clean.pkl')\n",
    "documents = df['description'].str.join(' ')\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-568756767a36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m vectorizer = TfidfVectorizer(ngram_range=(1, 2),\n\u001b[0m\u001b[1;32m      3\u001b[0m                              \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.55\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                              \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                              \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "num_features = 1000\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2),\n",
    "                             max_df=0.55,\n",
    "                             max_features=num_features,\n",
    "                             token_pattern=None,\n",
    "                             tokenizer=tokenize_and_lemmatize)\n",
    "# LDA can only uses raw term counts\n",
    "tf = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_feature_names = vectorizer.get_feature_names() #theses are the words in our bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, max_iter=5, learning_method='online',random_state=42, n_jobs=-1)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = lda.components_[0]\n",
    "print(temp.shape)\n",
    "temp.argsort()[:10].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the top ten words for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_feature_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-bce44b1dcf39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnum_top_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_feature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_top_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tf_feature_names' is not defined"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "\n",
    "num_top_words = 10\n",
    "display_topics(lda, tf_feature_names, num_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "\n",
    "Model [perplexity](https://en.wikipedia.org/wiki/Perplexity) is often used in LDA to evaluate how well a model predicts a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model perplexity: {0:0.3f}\".format(lda.perplexity(tf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show how to do LDA in gensim\n",
    "\n",
    "Example from https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "\n",
    "If you don't already have gensim installed:  \n",
    "`$ pip install -U gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = data_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = df['description'].apply(tokenize_and_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary\n",
    "id2word = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "#create corpus\n",
    "texts = processed_docs\n",
    "\n",
    "#Term Document Frequency\n",
    "bow_corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=id2word, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View topics in the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Model Perplexity and Coherence Score (interpretability of the model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(bow_corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the topics-keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbasecondae777eab680444837bb85667e680297d8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}